<!doctype html>
<html lang=en>
<head>
<style type="text/css">
.inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
}
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.left {
  text-align: left;
}
.right {
  text-align: right;
}
.center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
<meta charset=utf-8>
<title>Stefan Siegert</title>
<link rel=stylesheet href=/style.css>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<h1><a href=/ title="Stefan Siegert">Stefan Siegert</a></h1>


<h2>A brief introduction to propagation of uncertainty</h2>





<p>Assume we have a scalar function of several inputs \(f(x,y,z,...)\). We want to look at the different ways in which randomness in the input variables \(x, y, ...\) translates into randomness in the function value. This principle is of fundamental importance when analyzing how uncertainties in input parameters and measurements propagate through mathematical models to impact the output.

<h2>Analytical methods</h2>

<h3>Univariate functions</h3>

<p>Let's first consider univariate functions, i.e. \(f(x)\), where \(f\) is a known function. For a random variable \(X\) with a known distribution, applying a transformation \(Y=f(X)\) results in a new random variable \(Y\), whose distribution and statistical properties (such as expectation and variance) can be derived from those of \(X\).

<p>Transformation of Expectation: \(E[Y]\) can be calculated directly for a linear transformation \(f(x) = ax + b\): $$E[Y] = E[aX + b)] = aE[X] + b = f(E[X]).$$ When \(f\) is non-linear, we can use the Law of the Unconscious Statistician (LOTUS) to calculate the expectation of \(f(X)\): $$E[Y] = E[f(X)] = \int f(x) p(x) dx$$ where the integral is over the domain of \(X\) and \(p(x)\) is the pdf of \(X\). The integral can be solved analytically if \(f(x)p(x)\) is sufficiently simple. Otherwise we have to use numerical methods.

<p>Transformation of Variance: \(Var[Y]\) can be calculated directly for linear transformations \(f(x) = ax + b\): $$Var[Y] = Var[aX + b] = a^2 Var[X].$$ When \(f\) is nonlinear but not too complicated, we might be able to use the LOTUS to calculate the variance of \(Y\) directly via $$\begin{aligned}Var[Y] & = E[Y^2] - E[Y]^2 = E[f(X)^2] - E[f(X)]^2\\ & = \int f(x)^2 p(x) dx - \left[ \int f(x) p(x) dx \right]^2.\end{aligned}$$ If that can't be solved we have to use some sort of approximations.

<p>Transformation of the Whole Distribution: The distribution of \(Y\) can be derived from the distribution of \(X\) using the change of variables technique, especially when the function \(f(x)\) is monotonic.

<p>Delta method: The Delta Method is used to approximate the variance of a transformed random variable \(Y = f(X)\). It is particularly useful when dealing with non-linear differentiable transformations \(f(x)\). To apply the Delta Method, the expectation \(\mu\) and variance \(\sigma^2\) of the untransformed variable \(X\) must be known. We first Taylor-expand \(f(X)\) around \(\mu\): \(Y = f(X) \approx f(\mu) + f'(\mu) (X - \mu)\) where \(f'(\mu)\) denotes the derivative of \(f\) evaluated at \(\mu\). Under this approximation, the expectation of \(Y\) is \(f(\mu)\). The Taylor approximation of \(Y\) is a linear function of \(X\) so the approximate variance of \(Y\) is given by \(Var[Y] \approx [f'(\mu)]^2 \sigma^2\).


<h3>Multivariate functions</h3>

<p>When the transformation \(f\) is multivariate, this means that the transformed random variable \(Y\) depends on several random variables \(X_1, X_2, \dots\). We merge them into a random vector \(X = (X_1, X_2, \dots)^T\) which has an expectation vector \(\mu = (\mu_1, \mu_2, \dots)^T\), a variance-covariance matrix \(\Sigma\) has elements \(\Sigma_{ij} = Cov[X_i, X_j]\), and joint pdf \(p(X) = p(X_1, X_2, \dots)\). 

<p>When the transformation \(Y=f(X)\) is linear we have \(f(X) = AX + b\) where \(A\) is a matrix with one row and b is a scalar (remember that we assume that \(Y\) is scalar). The expectation of \(Y\) is then given by \(E[Y] = A\mu + b\) and its variance is given by \(Var[Y] = A\Sigma A^T\). The pdf of \(Y\) can be derived from the change of variable formula. MORE HERE

<p>When the transformation \(Y=f(X)\) is non-linear, we can use the Delta method. Let \(\nabla f\) denote the gradient vector of \(f\), that is, the \(i\)th element of \(\nabla f\) is the derivative of \(f\) with respect to its \(i\)th argument. Then the variance of \(Y = f(X)\) is approximated by $$Var[Y] \approx \nabla f(\mu)^T \Sigma \nabla f(\mu).$$


<h2>Monte Carlo</h2>

<p>The foundation for Monte Carlo methods is the Law of Large Numbers, which states that as the number of trials in a random experiment increases, the average of the results from all trials converges to the expected value. This principle underpins the reliability of Monte Carlo simulations for uncertainty propagation: By simulating a system many times with randomly sampled input parameters, we can approximate the distribution of the output, including its variance and other statistical properties.

<p>Monte Carlo methods involve generating a large number of realisations of input variables sampled from their respective distributions. Then we calculate the output of the transformation for each sample of input variables. The collection of outputs then provides a statistical sample of possible outcomes. From this we can derive the empirical distribution, as well as expectation, variance, etc of the output variable. The Monte-Carlo approach is particularly valuable for non-linear transformations, transformations that are not given in mathematical closed form.

Below is an R code chunk that demonstrates a Monte Carlo simulation where \(X_1\) is initial velocity and \(X_2\) is initial angle of a projectile fired from ground level. The output \(Y\) is the maximum height of the projectile, given by \[Y = f(X_1, X_2) = \frac{X_1^2 \sin^2(X_2 \pi / 180)}{2g}\] where \(g = 9.81\) is the gravitational acceleration. If we are uncertain about \(X_1\) and \(X_2\), we are also uncertainty about \(Y\). We will describe our uncertainty in the velocity \(X_1\) by a Normal distribution, and uncertainty in the angle \(X_2\) by a Uniform distribution. 


<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(tidyverse)</span>
</pre></div>
</div></div>
<div class="chunk" id="mc-histogram"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl com"># simulate random inputs </span>
<span class="hl std">X1</span> <span class="hl kwb">=</span> <span class="hl kwd">rnorm</span><span class="hl std">(</span><span class="hl num">1e4</span><span class="hl std">,</span> <span class="hl kwc">mean</span> <span class="hl std">=</span> <span class="hl num">30</span><span class="hl std">,</span> <span class="hl kwc">sd</span> <span class="hl std">=</span> <span class="hl num">2</span><span class="hl std">)</span>  <span class="hl com"># Initial velocity (m/s) </span>
<span class="hl std">X2</span> <span class="hl kwb">=</span> <span class="hl kwd">runif</span><span class="hl std">(</span><span class="hl num">1e4</span><span class="hl std">,</span> <span class="hl kwc">min</span> <span class="hl std">=</span> <span class="hl num">30</span><span class="hl std">,</span> <span class="hl kwc">max</span> <span class="hl std">=</span> <span class="hl num">40</span><span class="hl std">)</span>  <span class="hl com"># Projection angle (degrees) </span>
<span class="hl com"># calculate output</span>
<span class="hl std">Y</span> <span class="hl kwb">=</span> <span class="hl std">(X1</span><span class="hl opt">^</span><span class="hl num">2</span> <span class="hl opt">*</span> <span class="hl kwd">sin</span><span class="hl std">(X2</span><span class="hl opt">*</span><span class="hl std">pi</span><span class="hl opt">/</span><span class="hl num">180</span><span class="hl std">)</span><span class="hl opt">^</span><span class="hl num">2</span><span class="hl std">)</span> <span class="hl opt">/</span> <span class="hl std">(</span><span class="hl num">2</span> <span class="hl opt">*</span> <span class="hl num">9.81</span><span class="hl std">)</span>

<span class="hl com"># Plot the density histogram of the output</span>
<span class="hl kwd">ggplot</span><span class="hl std">()</span> <span class="hl opt">+</span> <span class="hl kwd">geom_histogram</span><span class="hl std">(</span><span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">x</span><span class="hl std">=Y,</span> <span class="hl kwc">y</span><span class="hl std">=</span><span class="hl kwd">after_stat</span><span class="hl std">(density)),</span>
                          <span class="hl kwc">binwidth</span> <span class="hl std">=</span> <span class="hl num">1</span><span class="hl std">,</span> <span class="hl kwc">fill</span> <span class="hl std">=</span> <span class="hl str">&quot;blue&quot;</span><span class="hl std">)</span>
</pre></div>
</div><div class="rimage default"><img src="fig/uncprop.Rhtml-mc-histogram-1.png" alt="plot of chunk mc-histogram" class="plot" /></div><div class="rcode">
<div class="source"><pre class="knitr r"><span class="hl kwd">print</span><span class="hl std">(</span><span class="hl kwd">c</span><span class="hl std">(</span><span class="hl kwc">`mean(Y)`</span> <span class="hl std">=</span> <span class="hl kwd">mean</span><span class="hl std">(Y),</span> <span class="hl kwc">`sd(Y)`</span><span class="hl std">=</span><span class="hl kwd">sd</span><span class="hl std">(Y)))</span>
</pre></div>
<div class="output"><pre class="knitr r">##  mean(Y)    sd(Y) 
## 15.22099  3.00688
</pre></div>
</div></div>

As next steps in studying uncertainty propagation, here are some questions we might ask to move forward:

<ul>
  <li>Sometimes the distributions of input variables aren't clearly defined. How should expectations, variances or distributions of input variables be specified when we have no or only imprecise knowledge about their uncertainty? 	
<p>&rarr; <a href=https://en.wikipedia.org/wiki/Expert_elicitation target="_blank">Expert elicitation</a>
  <li>Calculation of the output might be computationally expensive, so generating 1000 Monte Carlo samples is not possible. What should we do? <p>&rarr; <a href=https://en.wikipedia.org/wiki/Surrogate_model target="_blank">Surrogate modelling</a>
  <li>How can we quantify how much each of the inputs affects uncertainty in the output? Is the output more sensitive to some inputs than to others? <p>&rarr; <a href=https://en.wikipedia.org/wiki/Sensitivity_analysis target="_blank">Sensitivity analysis</a>
  <li>Can we explore the space of input variables more efficiently than by sampling randomly and independently? <p>&rarr; <a href=https://en.wikipedia.org/wiki/Design_of_experiments target="_blank">Design of Experiments</a>
</ul> 





