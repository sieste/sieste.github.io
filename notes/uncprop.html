<!doctype html>
<html lang=en>
<head>
<style type="text/css">
.inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
}
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.left {
  text-align: left;
}
.right {
  text-align: right;
}
.center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
<meta charset=utf-8>
<title>Stefan Siegert</title>
<link rel=stylesheet href=/style.css>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<h1><a href=/ title="Stefan Siegert">Stefan Siegert</a></h1>


<h2>Propagating uncertainty from inputs to outputs of a function</h2>





<p>Assume we have a scalar function of several inputs \(f(x_1,x_2,\dots)\). In this note I'll look at different ways to quantify how randomness or uncertainty in the input variables \(x_1, x_2, ...\) translates into randomness or uncertainty in the function value. 

<h2>Exact methods</h2>

<h3>Univariate functions</h3>

<p>Let's first consider univariate functions, i.e. \(y=f(x)\), where \(f\) is a known function. For a random variable \(X\) with a known distribution, applying a transformation \(f(X)\) results in a new random variable \(Y\) whose distribution and statistical properties (such as expectation and variance) can (sometimes) be derived from those of \(X\).

<p>Transformation of Expectation: Expectation is not a measure of uncertainty, but is important enough for us to include here anyway. \(E[Y]\) can be calculated directly when the transformation is linear \(f(x) = ax + b\): $$E[Y] = E[aX + b)] = aE[X] + b = f(E[X]).$$ 

<p>When the transformation \(f\) is non-linear, we can sometimes use the Law of the Unconscious Statistician (LOTUS) to calculate the expectation of \(Y=f(X)\). When \(p(x)\) is the pdf of \(X\) we have $$E[Y] = E[f(X)] = \int f(x) p(x) dx$$ where the integral is over the domain of \(X\). However, integrating \(f(x)p(x)\) analytically might not be possible.

<p>Transformation of Variance: \(Var[Y]\) can be calculated directly for linear transformations \(f(x) = ax + b\): $$Var[Y] = Var[aX + b] = a^2 Var[X].$$ 

When \(f\) is nonlinear but not too "complicated", we can use the LOTUS to also calculate the variance of \(Y\): $$\begin{aligned}Var[Y] & = E[Y^2] - E[Y]^2 = E[f(X)^2] - E[f(X)]^2\\ & = \int f(x)^2 p(x) dx - \left[ \int f(x) p(x) dx \right]^2.\end{aligned}$$ As before, depending on the transformation and pdf the integrals might not be tractable.

<p>Transformation of the Whole Distribution: The whole pdf of \(Y\) can be derived from the distribution of \(X\) using the change of variables technique. For simplicity we will assume here that \(f(x)\) is monotonic (either strictly increasing or strictly decreasing), and differentiable over the range of \(X\). We assume that the inverse transformation \(x = f^{-1}(y)\) is known. The pdf of \(Y\), denoted \(q(y)\) is then given by $$q(y) = \left| \frac{d}{dy}f^{-1}(y)\right| p(f^{-1}(y))$$



<h3>Multivariate functions</h3>

<p>When the transformation \(f\) is multivariate, this means that the transformed random variable \(Y\) depends on several random variables \(X_1, X_2, \dots\). We merge them into a random vector \(X = (X_1, X_2, \dots)^T\) which has an expectation vector \(\mu = (\mu_1, \mu_2, \dots)^T\), a variance-covariance matrix \(\Sigma\) has elements \(\Sigma_{ij} = Cov[X_i, X_j]\), and joint pdf \(p(X) = p(X_1, X_2, \dots)\). 

<p>When the transformation \(Y=f(X)\) is linear we have \(f(X) = AX + b\) where \(A\) is a matrix with one row and b is a scalar. The expectation of \(Y\) is then given by \[E[Y] = A\mu + b\] and its variance is given by \[Var[Y] = A\Sigma A^T\]. The pdf \(q(y)\) of \(Y\) can be derived from \(p(x)\) by the change of variable formula \[q(y) = \int_{\mathbb{R}^n}p(x)\delta(y - f(x))dx.\] More background can be found on <a href=https://en.wikipedia.org/wiki/Probability_density_function#Vector_to_scalar target="_blank">wikipedia</a>. But I have not seen change of variable being used much for uncertainty propagation so won't go into detail right now.


<h2>Delta method</h2>

<p>Delta method: The Delta Method is used to approximate the variance of a transformed random variable \(Y = f(X)\). It is particularly useful when dealing with non-linear differentiable transformations \(f(x)\). To apply the Delta Method, the expectation \(\mu\) and variance \(\sigma^2\) of the untransformed variable \(X\) must be known. We first Taylor-expand \(f(X)\) around \(\mu\): \(Y = f(X) \approx f(\mu) + f'(\mu) (X - \mu)\) where \(f'(\mu)\) denotes the derivative of \(f\) evaluated at \(\mu\). Under this approximation, the expectation of \(Y\) is \(f(\mu)\). The Taylor approximation of \(Y\) is a linear function of \(X\) so the approximate variance of \(Y\) is given by \[Var[Y] \approx [f'(\mu)]^2 \sigma^2.\]

<p>The Delta method is also applicable to multivariate functions \(Y=f(X_1, X_2, \dots)\). Let \(\nabla f(x)\) denote the gradient of \(f\) evaluated at the vector \(x=(x_1,x_2,\dots)^T\). Then the variance of \(Y = f(X)\) is approximated by $$Var[Y] \approx \nabla f(\mu)^T \Sigma \nabla f(\mu).$$ 


<h2>Monte Carlo simulation</h2>

<p>The foundation for Monte Carlo methods is the Law of Large Numbers, which states that as the number of trials in a random experiment increases, the average of the results from all trials converges to the expected value. This principle underpins the reliability of Monte Carlo simulations for uncertainty propagation: By simulating a system many times with randomly sampled input parameters, we can approximate the distribution of the output, including its variance and other statistical properties.

<p>Monte Carlo methods involve generating a large number of realisations of input variables sampled from their respective distributions. Then we calculate the output of the transformation for each sample of input variables. The collection of outputs then provides a statistical sample of possible outcomes. From this we can derive the empirical distribution, as well as expectation, variance, etc of the output variable. The Monte-Carlo approach is particularly valuable for non-linear transformations, transformations that are not given in mathematical closed form.

Below is an R code chunk that demonstrates a Monte Carlo simulation where \(X_1\) is initial velocity and \(X_2\) is initial angle of a projectile fired from ground level. The output \(Y\) is the maximum height of the projectile, given by \[Y = f(X_1, X_2) = \frac{X_1^2 \sin^2(X_2 \pi / 180)}{2g}\] where \(g = 9.81\) is the gravitational acceleration. If we are uncertain about \(X_1\) and \(X_2\), we are also uncertainty about \(Y\). We will describe our uncertainty in the velocity \(X_1\) by a Normal distribution, and uncertainty in the angle \(X_2\) by a Uniform distribution. 


<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(tidyverse)</span>
</pre></div>
</div></div>
<div class="chunk" id="mc-histogram"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl com"># simulate random inputs </span>
<span class="hl std">X1</span> <span class="hl kwb">=</span> <span class="hl kwd">rnorm</span><span class="hl std">(</span><span class="hl num">1e4</span><span class="hl std">,</span> <span class="hl kwc">mean</span> <span class="hl std">=</span> <span class="hl num">30</span><span class="hl std">,</span> <span class="hl kwc">sd</span> <span class="hl std">=</span> <span class="hl num">2</span><span class="hl std">)</span>  <span class="hl com"># Initial velocity (m/s) </span>
<span class="hl std">X2</span> <span class="hl kwb">=</span> <span class="hl kwd">runif</span><span class="hl std">(</span><span class="hl num">1e4</span><span class="hl std">,</span> <span class="hl kwc">min</span> <span class="hl std">=</span> <span class="hl num">30</span><span class="hl std">,</span> <span class="hl kwc">max</span> <span class="hl std">=</span> <span class="hl num">40</span><span class="hl std">)</span>  <span class="hl com"># Projection angle (degrees) </span>
<span class="hl com"># calculate output</span>
<span class="hl std">Y</span> <span class="hl kwb">=</span> <span class="hl std">(X1</span><span class="hl opt">^</span><span class="hl num">2</span> <span class="hl opt">*</span> <span class="hl kwd">sin</span><span class="hl std">(X2</span><span class="hl opt">*</span><span class="hl std">pi</span><span class="hl opt">/</span><span class="hl num">180</span><span class="hl std">)</span><span class="hl opt">^</span><span class="hl num">2</span><span class="hl std">)</span> <span class="hl opt">/</span> <span class="hl std">(</span><span class="hl num">2</span> <span class="hl opt">*</span> <span class="hl num">9.81</span><span class="hl std">)</span>

<span class="hl com"># Plot the density histogram of the output</span>
<span class="hl kwd">ggplot</span><span class="hl std">()</span> <span class="hl opt">+</span> <span class="hl kwd">geom_histogram</span><span class="hl std">(</span><span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">x</span><span class="hl std">=Y,</span> <span class="hl kwc">y</span><span class="hl std">=</span><span class="hl kwd">after_stat</span><span class="hl std">(density)),</span>
                          <span class="hl kwc">binwidth</span> <span class="hl std">=</span> <span class="hl num">1</span><span class="hl std">,</span> <span class="hl kwc">fill</span> <span class="hl std">=</span> <span class="hl str">&quot;blue&quot;</span><span class="hl std">)</span>
</pre></div>
</div><div class="rimage default"><img src="fig/uncprop.Rhtml-mc-histogram-1.png" alt="plot of chunk mc-histogram" class="plot" /></div><div class="rcode">
<div class="source"><pre class="knitr r"><span class="hl kwd">print</span><span class="hl std">(</span><span class="hl kwd">c</span><span class="hl std">(</span><span class="hl kwc">`mean(Y)`</span> <span class="hl std">=</span> <span class="hl kwd">mean</span><span class="hl std">(Y),</span> <span class="hl kwc">`var(Y)`</span><span class="hl std">=</span><span class="hl kwd">var</span><span class="hl std">(Y),</span> <span class="hl kwc">`sd(Y)`</span><span class="hl std">=</span><span class="hl kwd">sd</span><span class="hl std">(Y)))</span>
</pre></div>
<div class="output"><pre class="knitr r">##   mean(Y)    var(Y)     sd(Y) 
## 15.195385  8.819005  2.969681
</pre></div>
</div></div>

Let's compare the Monte Carlo variance to the delta method approximation. We have the joint expectation of \((X_1,X_2)^T\) as \(\mu = (30,35)^T\) and since \(X_1\) and \(X_2\) are independent their variance matrix is a diagonal matrix with \(diag(\Sigma) = (4, 100/12)^T\). I'll use R's built-in automatic symbolic differentiation function "deriv" to calculate the gradient. "deriv" can calculate derivatives of "simple" functions analytically. However, it has a very strange syntax: It takes the function to be differentiated as an R formula (starting with ~), then returns a function, which returns the desired gradient vector as an attribute of its return value. Anyway, this is the automatic differentiation version of the delta method, which should be quite straightforward to port to different functions:

<div class="chunk" id="unnamed-chunk-3"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">f</span> <span class="hl kwb">=</span> <span class="hl opt">~</span><span class="hl std">x1</span><span class="hl opt">^</span><span class="hl num">2</span><span class="hl opt">*</span><span class="hl kwd">sin</span><span class="hl std">(x2</span><span class="hl opt">*</span><span class="hl std">pi</span><span class="hl opt">/</span><span class="hl num">180</span><span class="hl std">)</span><span class="hl opt">^</span><span class="hl num">2</span><span class="hl opt">/</span><span class="hl std">(</span><span class="hl num">2</span><span class="hl opt">*</span><span class="hl num">9.81</span><span class="hl std">)</span>
<span class="hl std">deriv_f</span> <span class="hl kwb">=</span> <span class="hl kwd">deriv</span><span class="hl std">(f,</span> <span class="hl kwd">c</span><span class="hl std">(</span><span class="hl str">'x1'</span><span class="hl std">,</span><span class="hl str">'x2'</span><span class="hl std">),</span> <span class="hl kwa">function</span><span class="hl std">(</span><span class="hl kwc">x1</span><span class="hl std">,</span><span class="hl kwc">x2</span><span class="hl std">){})</span>
<span class="hl std">grad_f</span> <span class="hl kwb">=</span> <span class="hl kwa">function</span><span class="hl std">(</span><span class="hl kwc">x1</span><span class="hl std">,</span> <span class="hl kwc">x2</span><span class="hl std">)</span>
  <span class="hl kwd">as.vector</span><span class="hl std">(</span><span class="hl kwd">attributes</span><span class="hl std">(</span><span class="hl kwd">deriv_f</span><span class="hl std">(x1, x2))</span><span class="hl opt">$</span><span class="hl std">gradient)</span>
<span class="hl std">grad_f_mu</span> <span class="hl kwb">=</span> <span class="hl kwd">grad_f</span><span class="hl std">(</span><span class="hl num">30</span><span class="hl std">,</span> <span class="hl num">35</span><span class="hl std">)</span>
<span class="hl std">Sigma</span> <span class="hl kwb">=</span> <span class="hl kwd">diag</span><span class="hl std">(</span><span class="hl kwd">c</span><span class="hl std">(</span><span class="hl num">4</span><span class="hl std">,</span><span class="hl num">100</span><span class="hl opt">/</span><span class="hl num">12</span><span class="hl std">))</span>
<span class="hl kwd">drop</span><span class="hl std">(</span><span class="hl kwd">t</span><span class="hl std">(grad_f_mu)</span> <span class="hl opt">%*%</span> <span class="hl std">Sigma</span> <span class="hl opt">%*%</span> <span class="hl std">grad_f_mu)</span>
</pre></div>
<div class="output"><pre class="knitr r">## [1] 8.765465
</pre></div>
</div></div>

This is reassuringly close to the Monte Carlo estimate of the variance.


<h2>Next steps</h2>

As next steps in studying uncertainty propagation, here are some questions we might ask to move forward:

<ul>
  <li>Sometimes the distributions of input variables aren't clearly defined. How should expectations, variances or distributions of input variables be specified when we have no or only imprecise knowledge about their uncertainty? 	
<p>&rarr; <a href=https://en.wikipedia.org/wiki/Expert_elicitation target="_blank">Expert elicitation</a>
  <li>Calculation of the output might be computationally expensive, so generating 1000 Monte Carlo samples is not possible. What should we do? <p>&rarr; <a href=https://en.wikipedia.org/wiki/Surrogate_model target="_blank">Surrogate modelling</a>
  <li>How can we quantify how much each of the inputs affects uncertainty in the output? Is the output more sensitive to some inputs than to others? <p>&rarr; <a href=https://en.wikipedia.org/wiki/Sensitivity_analysis target="_blank">Sensitivity analysis</a>
  <li>Can we explore the space of input variables more efficiently than by sampling randomly and independently? <p>&rarr; <a href=https://en.wikipedia.org/wiki/Design_of_experiments target="_blank">Design of Experiments</a>
</ul> 





