<!doctype html>
<html lang=en>
<head>
<meta charset=utf-8>
<title>Stefan Siegert</title>
<link rel=stylesheet href=/style.css>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<h1><a href=/ title="Stefan Siegert">Stefan Siegert</a></h1>


<!--
:nnoremap = :!R -e "knitr::knit('%')"<enter><enter>:!xdotool key --window $(xdotoo
l search --name "Mozilla Firefox") F5<enter><enter>
-->

<!--begin.rcode init, echo=FALSE
  set.seed(123)
  prefix = paste('fig/', knitr::current_input(dir=FALSE), '-', sep='')
  knitr::opts_chunk$set(fig.width=7, fig.height=7, fig.path=prefix)
end.rcode-->


<h2>Coffee temperature data</h2>

I have taken a freshly brewed cup of coffee and starting measuring its temperature with a kitchen thermometer every minute until I got bored. This is the data and a time series plot:

<!--begin.rcode 

coffee_data = 
tibble(t=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 
           10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 
           20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
           30, 31, 32, 33, 34, 35, 36, 37, 38, 39,
           40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 
           50, 51, 52), 
       T = c(193.3, 178.5, 178.5, 174, 169.5, 166.3, 163.9, 161.6, 159.4, 
             156.7, 154.9, 151.5, 149.2, 147.6, 145.9, 143.1, 142.0, 140.2, 138.4, 
             136.8, 135.0, 133.0, 131.5, 130.3, 128.5, 127.4, 126.7, 124.9, 124.7,
             122.9, 122.2, 120.7, 119.8, 118.9, 117.0, 115.5, 115.2, 113.5, 113.0, 
             111.7, 110.7, 110.7, 109.8, 109.0, 108.3, 106.9, 106.7, 105.8, 105.3, 
             105.1, 104.0, 104.2)) |>
mutate(T = (T - 32)/1.8)

coffee_data |>
ggplot() + geom_point(aes(x=t, y=T)) + ylim(0,100)

end.rcode-->


<h2>Newtonian cooling</h2>

A physical description of the cooling process is that the rate of temperature change is proportional to difference from environment's temperature, alson known as Newtonian cooling:

\[\frac{dT}{dt} = -k(T - T_E)\]

This differential equation has an analytic solution \(T(t)\) of the form

\[T(t) = T_E + (T_0 - T_E)e^{-kt}\]

where \(T_0\), \(T_E\) and \(k\) are parameters. If analytic solution cannot be derived (which is usually the case) we can try to solve the differential equation numerically.

<p>Below is a function definition and a few plots of \(T(t)\) for different parameter settings:

<!--begin.rcode 
Tfun = function(t, pars) {
  return(pars[2] + (pars[3] - pars[2])*exp(-pars[1]*t))
}
end.rcode-->


<!--begin.rcode 
library(tidyverse)
end.rcode-->

<!--begin.rcode

pars = tibble(k = c(1, 2, 3), Te = c(20, 30, 40), T0 = c(100, 90, 80))
tt = seq(0, 7, .1)
pars |> 
  rowwise() |> 
  mutate(T = list(Tfun(tt, c(k, Te, T0))), t=list(tt)) |> 
  unnest(cols=c(T,t)) |> 
  mutate(parstr = paste('k=',k,' Te=', Te, ' T0=', T0, sep='')) |>
  ggplot() + 
    geom_line(aes(x=t, y=T, col=parstr)) +
    labs(col=NULL) 

end.rcode-->

<p>The parameters \(T_0\) and \(T_E\) are easy to interpret, they are the starting temperature at \(t=0\) and the asymptotic end temperature for \(t \to \infty\). The cooling rate \(k\) determines how quickly \(T_E\) is approached: the higher \(k\) the quicker the cooling.


<h2>Using the cooling coffee cup as a thermometer</h2>

Since \(T_E\) is the temperature of the environment, I should be able to use the data to find the best fitting curve and the best fitting value of \(T_E\) will tell me the temperature of my kitchen during the experiment. Hence I can use the coffee cup as a room thermometer.


<h2>Optimisation</h2>

Formally we address this problem by finding the triplet of parameters \((T_0, T_E, k)\) that give a temperature curve for which the sum of squared differences to the observed data is minimised. There is no analytic solution, but we can use numerical optimisation:

<!--begin.rcode 
# sum of squares function
ss = function(pars) sum((coffee_data$T - Tfun(coffee_data$t, pars))^2)
# numerical optimisation, starting from "reasonable" initial values
ss_opt = optim(c(1,30,100), ss)
print(ss_opt)
end.rcode-->

So my model tells me that the best fitting room temperature for my data is about 33C. The data were taken in late February in the UK. Windows and general insulation are up to UK standards, which means terrible, so even if I wanted to heat my house to 33C I wouldn't be able to. This result is clearly wrong. But look how nicely the model fits the data:

<!--begin.rcode 
tt = seq(0,180,1)
tibble(t = tt, T = Tfun(tt, ss_opt$par)) |>
ggplot(aes(x=t, y=T)) + 
  geom_line() + geom_point(data=coffee_data) +
  geom_hline(yintercept=ss_opt$par[2], lty=2) + ylim(0, 100)
end.rcode-->

At least two things could be happening here: Either the data is bad, or my model is wrong (or both). I might have also made a mistake in my coding but that never happens so it has to be either the data's or the model's fault. Or both. Let's look at these individually.


<h2>Likelihood inference</h2>

On the back of my kitchen thermometer there is a small sticker saying that the precision of the thermometer is 0.2C. I interpret this loosely as meaning that the observed temperature is normally distributed around the "true" value with mean zero and standard deviation \(0.2\):

\[\hat{T}(t) \sim N(T(t), 0.2^2)\]

This defines a log-likelihood function for my parameters given by

\[\begin{aligned}
\ell(T_0,T_E,k) & = \sum_{i=1}^n \log p(\hat{T}(t_i) | T_0, T_E, k)\\
& = C -\frac{1}{2 \times 0.2^2} \sum_{i=1}^n \left[(\hat{T}(t_i) - (T_E + (T_0 - T_E)e^{-kt})\right]^2
\end{aligned}\]

<p>where the constant \(C\) does not depend on any of the parameters.
We can maximise the log likelihood with respect to the model parameters to get the maximum likelihood estimates of the parameters. These will be exactly the same as the minimum sum of squares estimates we calculated earlier. However, likelihood inference also tells us about the uncertainty of the parameter estimates. Namely the vector of maximum likelihood estimates \(\hat\theta\) has a multivariate Normal distribution centered on the "true" parameter vector \(\theta\) and with covariance matrix given by the inverse expected information matrix. What this means in practice is that we can optimise the log likelihood as above, tell the optim function to also approximate the Hessian matrix, which is the matrix of second derivatives of the negative log likelihood, and use the inverse of the Hessian matrix as the covariance matrix that characterises the uncertainty of the parameter estimates. This is best explained through some actual code:



<!--begin.rcode 
library(mvtnorm)
# define negative log likelihood function up to an additive constant
nll = function(pars) {
  sum(0.5 * (coffee_data$T - Tfun(coffee_data$t, pars))^2 / 0.2^2 )
}
# use optim to minimise the negative log likelihood, and 
# also approximate the hessian matrix
nll_opt = optim(c(1,30,100), nll, hessian=TRUE)
print(nll_opt)
end.rcode-->


<!--begin.rcode 
# now draw samples from a multivariate normal distribution centered on the
# estimates, and with covariance given by the inverse hessian
par_sampl = rmvnorm(500, nll_opt$par, solve(nll_opt$hessian))
# plot the temperature curves corresponding to the sampled parameters
par_sampl |> 
  as.data.frame() |>
  `colnames<-`(c('k','Te', 'T0')) |>
  rowwise() |> 
  mutate(T = list(Tfun(tt, c(k, Te, T0))), t=list(tt)) |> 
  unnest(cols=c(T, t)) |> 
  mutate(parstr = paste('k=',k,' Te=', Te, ' T0=', T0, sep='')) |>
  ggplot(aes(x=t, y=T)) + 
    geom_line(aes(x=t, y=T, group=parstr), col='#00000022', show.legend=FALSE) +
    geom_point(data=coffee_data, col='red') +
    geom_segment(data=coffee_data, aes(x=t, xend=t, y=T-0.4, yend=T+0.4), col='red') +
    labs(col=NULL) 
end.rcode-->

I have put error bars with width 0.2 on the data points but they are barely visible. The measurement errors lead to some estimation uncertainty in the parameters including the asymptotic temperature \(T_E\). But all values are still well above 30C, so still unrealistic.



<h2>Bayesian inference</h2>

<p>Sampling from the multivariate Normal distribution with variance matrix given by the inverse hessian was actually an approximation of a Bayesian inference, called a Laplace approximation. The details of the approximation aren't important. What is more important is that Bayesian inference consists of the data as well as prior information. In the likelihood inference example we have assumed that we have absolutely no prior knowledge about the parameters, every region in parameter space is as likely as any other region in parameter space. But then we also said that 35C is clearly wrong, which shows that, actually, we do have prior knowledge. How else could we say that the inferred value is "unreasonable"?

<p>Bayesian inference now allows us to make that prior knowledge explicit in the form of a probability distribution over possible parameter values. Here I will only specify a prior for the parameter \(T_E\), but in general we could specify priors for the other parameters as well. My prior is that \(T_E\) is somewhere between 16C and 24C with values around 20C most likely. Formally I can translate this prior into a Normal distribution with mean 20 and standard deviation 2 \[T_E \sim N(20, 2^2)\]. My new function to maximise now becomes the posterior

\[\begin{aligned}
\log p(T_0,T_E,k|\hat{T}) & = C \times p(\hat{T}|T_0, T_E, k) \times p(T_E)\\
& = C' -\frac{1}{2 \times 0.2^2} \sum_{i=1}^n \left[(\hat{T}(t_i) - (T_E + (T_0 - T_E)e^{-kt})\right]^2 - \frac{1}{2 \times 2^2} (T_E - 20)^2
\end{aligned}\]

As before I can use the approximated Hessian matrix as the covariance matrix of the posterior distribution and sample from it:


<!--begin.rcode 
nlpost = function(pars) {
  sum(0.5 * (coffee_data$T - Tfun(coffee_data$t, pars))^2 / 0.2^2 + 
  0.5 * (pars[2]-20)^2 / 2^2)
}
nlpost_opt = optim(c(1,30,100), nlpost, hessian=TRUE)
print(nlpost_opt)
end.rcode-->

<!--begin.rcode 
par_sampl2 = rmvnorm(500, nlpost_opt$par, solve(nlpost_opt$hessian))
df_sampl2 = par_sampl2 |> 
  as.data.frame() |>
  `colnames<-`(c('k','Te','T0')) |>
  rowwise() |> 
  mutate(T = list(Tfun(tt, c(k, Te, T0))), t=list(tt)) |> 
  unnest(cols=c(T,t)) |> 
  mutate(parstr = paste('k=',k,' Te=', Te, ' T0=', T0, sep=''))

ggplot(df_sampl2, aes(x=t, y=T)) + 
  geom_line(aes(x=t, y=T, group=parstr), col='#00000022', show.legend=FALSE) +
  geom_point(data=coffee_data, col='red') +
  geom_segment(data=coffee_data, aes(x=t, xend=t, y=T-.4, yend=T+.4), col='red') +
  geom_hline(yintercept=c(16, 24), col='red', lty=2) +
  labs(col=NULL) 
end.rcode-->

Ok, so the prior on \(T_E\) pulled the fitted curves slightly away from the data towards \(T_E\) values that are a priori more "reasonable". The result is less ridiculous, but I'm still not happy. It just seems to warm, something is still wrong.


<h2>Model discprancy</h2>

Let's take a closer look at the difference between our fitted curves (with parameteres sampled from the posterior distribution) and the observed data.

<!--begin.rcode 

# visualise discrepancy
df_sampl1 |> 
  rename(Tsim = T) |> 
  right_join(coffee_data, by='t') |>
  filter(t > 2) |>
  mutate(dscrp = Tsim - T) |>
  ggplot() + 
    geom_line(aes(x=t, y=dscrp, group=parstr), 
              show.legend=FALSE, col='#00000011') +
    geom_hline(yintercept=0, lty=2)

end.rcode-->

<p>That plot is quite telling. What it shows is that there is a systematic difference between our best fitting curves and the data. It seems like there is simply no exponential function that can fit the measured data properly. In other words, Newtonian (exponential) cooling is not the correct model for out data. In the early stages of cooling the difference between fitted model and data is positive, meaning that the observed temperature is lower than the modelled one, that is, cooling is faster than expected by the model. And in later stages the discrepancy is negative, indicating that the cooling is slower than expected. And the differences are on the order of one degree, so they can't be attributed to measurement error which is only of size 0.2. More importantly, the difference between model and observation does not seem random but has a systematic pattern.

<p>A model to account for discrepancy


<!--begin.rcode 

# bayes with discrepancy
library(mvtnorm)
tt = seq(0,180,1)
nlpost = function(pars) sum(0.5 * (coffee_data$T - Tfun1(coffee_data$t, pars))^2 / 0.7^2 + 0.5 * (pars[2]-20)^2 / 3^2)
nlpost_opt = optim(c(1,30,100), nlpost, hessian=TRUE)
print(nlpost_opt)
print(sqrt(diag(solve(nlpost_opt$hessian))))
par_sampl = rmvnorm(500, nlpost_opt$par, solve(nlpost_opt$hessian))
par_sampl |> 
  as.data.frame() |>
  `colnames<-`(c('k','Te','T0')) |>
  rowwise() |> 
  mutate(T = list(Tfun1(tt, c(k, Te, T0))), t=list(tt)) |> 
  unnest(cols=c(T,t)) |> 
  mutate(parstr = paste('k=',k,' Te=', Te, ' T0=', T0, sep='')) |>
  ggplot(aes(x=t, y=T)) + 
    geom_line(aes(x=t, y=T, group=parstr), col='#00000022', show.legend=FALSE) +
    geom_point(data=coffee_data, col='red') +
    geom_segment(data=coffee_data, aes(x=t, xend=t, y=T-2.4, yend=T+2.4), col='red') +
    geom_hline(yintercept=c(16, 24), col='red', lty=2) +
    labs(col=NULL) 

end.rcode-->




