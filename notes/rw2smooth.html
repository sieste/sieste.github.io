<!doctype html>
<html lang=en>
<head>
<style type="text/css">
.inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
}
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.left {
  text-align: left;
}
.right {
  text-align: right;
}
.center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
<meta charset=utf-8>
<title>Stefan Siegert</title>
<link rel=stylesheet href=/style.css>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<h1><a href=/ title="Stefan Siegert">Stefan Siegert</a></h1>



<h2>Time series smoothing with the 2nd order random walk</h2>

<h3>The RW2 and its precision matrix</h3>

<p>
The 2nd order random walk (RW2) is a time series model defined by
\[x_t = 2 x_{t-1} - x_{t-2} + \epsilon_t\]
where \(\epsilon_t \sim N(0,\tau_x^{-1})\) is iid Normal with precision \(\tau_x\).

<p>The RW2 is a popular prior for temporally correlated data used in time series modelling and Bayesian hierarchical modelling.

<p>Realisations of the RW2 can be generated by realising that the second-order differences \((x_t - x_{t-1}) - (x_{t-1} - x_{t-2})\) are iid Normally distributed. The inverse operation of differencing is the cumulative sum, so one way to generate realisations of RW2 is to cum-summing iid Normal random variables twice.


<div class="chunk" id="unnamed-chunk-1"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">set.seed</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl std">)</span>
<span class="hl std">rw2</span> <span class="hl kwb">=</span> <span class="hl kwd">sapply</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl opt">:</span><span class="hl num">5</span><span class="hl std">,</span> <span class="hl kwa">function</span><span class="hl std">(</span><span class="hl kwc">ii</span><span class="hl std">)</span> <span class="hl kwd">cumsum</span><span class="hl std">(</span><span class="hl kwd">cumsum</span><span class="hl std">(</span><span class="hl kwd">rnorm</span><span class="hl std">(</span><span class="hl num">100</span><span class="hl std">))))</span>
<span class="hl kwd">matplot</span><span class="hl std">(rw2,</span> <span class="hl kwc">type</span><span class="hl std">=</span><span class="hl str">'l'</span><span class="hl std">,</span> <span class="hl kwc">lty</span><span class="hl std">=</span><span class="hl num">1</span><span class="hl std">)</span>
</pre></div>
</div><div class="rimage default"><img src="fig/rw2smooth.Rhtml-unnamed-chunk-1-1.png" title="plot of chunk unnamed-chunk-1" alt="plot of chunk unnamed-chunk-1" class="plot" /></div></div>


<p>
If the vector \(x = (x_1, x_2, \dots, x_n)\) is a realisation of RW2, we can show that \(x\) has a multivariate Normal distribution. The likelihood of \(x_3, \dots, x_n\), is given by

\[\begin{aligned}
\log p(x_3, \dots, x_n | x_1, x_2) & = \sum_{t=3}^n \log p(x_t|x_{t-1}, x_{t-2}, \tau_x)\\
& = const - \frac{\tau_x}{2} \sum_{t=3}^n (x_t - 2 x_{t-1} + x_{t-2})^2
\end{aligned}\]

The sum of squares can be written as the dot product of a vector \(z\) with itself, where here \(z\) is given by

\[z = D x = \begin{pmatrix}1 & -2 & 1\\& \ddots & \ddots & \ddots\end{pmatrix} x\]

Here \(z\) has length \(n-2\) and \(D\) has \(n-2\) rows and \(n\) columns.

Plugging this into the likelihood, we have

\[\begin{aligned}
\log p(x_3, \dots, x_n | x_1, x_2) & = \sum_{t=3}^n \log p(x_t|x_{t-1}, x_{t-2}, \tau_x)\\
& = const - \frac{1}{2} x'(\tau_x D'D)x\\
& = const - \frac{1}{2} x' Q_x x
\end{aligned}\]

Assuming that \(x_1\) and \(x_2\) have a joint uniform prior distribution, i.e. \(p(x_1, x_2| \tau_x) = const\), the full joint distribution of the vector \(x\) is that of a multivariate Normal distribution

\[\begin{aligned}
p(x|\tau_x) & = p(x_1, x_2 | \tau_x) \prod_{t=3}^n p(x_t|x_{t-1}, x_{t-2}, \tau_x)\\
& = const \times e^{-\frac12 x' Q_x x}
\end{aligned}\]

with precision (inverse variance) matrix given by

\[Q_x = \tau_x D'D = \tau_x \begin{pmatrix}
1 & -2 & 1\\
-2 & 5 & -4 & 1 \\
1 & -4 & 6 & -4 & 1\\
& \ddots & \ddots & \ddots & \ddots & \ddots\\
&& 1 & -4 & 6 & -4 & 1\\
&&& 1 & -4 & 5 & -2\\
&&&&1 & -2 & 1
\end{pmatrix}\]


<h3>The RW2 as a prior in Bayesian hierarchical modelling</h3>

Now assume we have time-series data \(y = (y_1, \dots, y_n)'\) that we want to model as the sum of a smooth latent vector \(x\) and independent Normally distributed noise \(e\) with variance \(\tau_e^{-1}\). We want to model the latent component \(x\) by an RW2. That is we have

\[y = x + e\]

where \(e \sim N(0, Q_e^{-1})\), where \(Q_e = \tau_e 1_n\) and \(1_n\) denotes the identity matrix, and \(x \sim N(0, Q_x^{-1})\) with the RW2 precision matrix \(Q_x\) as defined above. 

<p>
Let's first assume that \(\tau_e\) and \(\tau_x\) are known. How do we infer the latent vector \(x\) from the observed data \(y\)?

<p>
The joint log-density of \(x\) and \(y\) is given by

\[\begin{aligned}
\log p(x,y) & = \log p(y|x) + \log p(x)\\
& = const - \frac12 (y - x)'Q_e (y - x) - \frac12 x' Q_x x
\end{aligned}\]

We want to infer \(x\) from \(y\), that is we want to calculate \(p(x|y)\):

\[\begin{aligned}
\log p(x|y) & = const + \log p(x, y)\\
& = const - \frac12 x'(Q_x + Q_e)x + (Q_e y)'x\\
\end{aligned}\]

This is the density of the multivariate Normal distribution in <a href=/notes/mvncanon.html>canonical form</a>, and so the conditional distribution of \(x\) given \(y\) is multivariate Normal with conditional expectation

\[E(x|y) = (Q_x + Q_e)^{-1}Q_e y\]

and conditional variance

\[Var(x|y) = (Q_x + Q_e)^{-1}.\]

<h3>Implementation in R</h3>

<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(tidyverse)</span>
<span class="hl kwd">library</span><span class="hl std">(Matrix)</span>
</pre></div>
</div></div>


<div class="chunk" id="unnamed-chunk-4"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl com"># simulate latent x, errors, and observations</span>
<span class="hl kwd">set.seed</span><span class="hl std">(</span><span class="hl num">18</span><span class="hl std">)</span>
<span class="hl std">n</span> <span class="hl kwb">=</span> <span class="hl num">50</span>
<span class="hl std">tau_x</span> <span class="hl kwb">=</span> <span class="hl num">10</span>
<span class="hl std">tau_e</span> <span class="hl kwb">=</span> <span class="hl num">0.1</span>

<span class="hl std">xy</span> <span class="hl kwb">=</span> <span class="hl kwd">tibble</span><span class="hl std">(</span><span class="hl kwc">t</span><span class="hl std">=</span><span class="hl num">1</span><span class="hl opt">:</span><span class="hl std">n,</span>
            <span class="hl kwc">x</span><span class="hl std">=</span><span class="hl kwd">cumsum</span><span class="hl std">(</span><span class="hl kwd">cumsum</span><span class="hl std">(</span><span class="hl kwd">rnorm</span><span class="hl std">(n,</span> <span class="hl num">0</span><span class="hl std">,</span> <span class="hl num">1</span><span class="hl opt">/</span><span class="hl kwd">sqrt</span><span class="hl std">(tau_x)))),</span>
            <span class="hl kwc">y</span><span class="hl std">=x</span> <span class="hl opt">+</span> <span class="hl kwd">rnorm</span><span class="hl std">(n,</span> <span class="hl num">0</span><span class="hl std">,</span> <span class="hl num">1</span><span class="hl opt">/</span><span class="hl kwd">sqrt</span><span class="hl std">(tau_e)))</span>

<span class="hl kwd">ggplot</span><span class="hl std">(xy,</span> <span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">x</span><span class="hl std">=t))</span> <span class="hl opt">+</span>
  <span class="hl kwd">geom_line</span><span class="hl std">(</span><span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">y</span><span class="hl std">=x,</span> <span class="hl kwc">colour</span><span class="hl std">=</span><span class="hl str">'latent x'</span><span class="hl std">))</span> <span class="hl opt">+</span>
  <span class="hl kwd">geom_line</span><span class="hl std">(</span><span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">y</span><span class="hl std">=y,</span> <span class="hl kwc">colour</span><span class="hl std">=</span><span class="hl str">'observed data y'</span><span class="hl std">))</span>
</pre></div>
</div><div class="rimage default"><img src="fig/rw2smooth.Rhtml-unnamed-chunk-4-1.png" title="plot of chunk unnamed-chunk-4" alt="plot of chunk unnamed-chunk-4" class="plot" /></div></div>


<div class="chunk" id="unnamed-chunk-5"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl com"># define precision matrices Q_e and Q_x </span>
<span class="hl std">Qe</span> <span class="hl kwb">=</span> <span class="hl std">tau_e</span> <span class="hl opt">*</span> <span class="hl kwd">Diagonal</span><span class="hl std">(n)</span>
<span class="hl std">Dx</span> <span class="hl kwb">=</span> <span class="hl kwd">bandSparse</span><span class="hl std">(</span><span class="hl kwc">n</span><span class="hl std">=n</span><span class="hl opt">-</span><span class="hl num">2</span><span class="hl std">,</span> <span class="hl kwc">m</span><span class="hl std">=n,</span> <span class="hl kwc">k</span><span class="hl std">=</span><span class="hl num">0</span><span class="hl opt">:</span><span class="hl num">2</span><span class="hl std">,</span>
                <span class="hl kwc">diagonal</span><span class="hl std">=</span><span class="hl kwd">list</span><span class="hl std">(</span><span class="hl kwd">rep</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl std">, n</span><span class="hl opt">-</span><span class="hl num">2</span><span class="hl std">),</span> <span class="hl kwd">rep</span><span class="hl std">(</span><span class="hl opt">-</span><span class="hl num">2</span><span class="hl std">, n</span><span class="hl opt">-</span><span class="hl num">2</span><span class="hl std">),</span> <span class="hl kwd">rep</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl std">, n</span><span class="hl opt">-</span><span class="hl num">2</span><span class="hl std">)))</span>
<span class="hl std">Qx</span> <span class="hl kwb">=</span> <span class="hl std">tau_x</span> <span class="hl opt">*</span> <span class="hl kwd">crossprod</span><span class="hl std">(Dx)</span>
<span class="hl std">y</span> <span class="hl kwb">=</span> <span class="hl std">xy</span><span class="hl opt">$</span><span class="hl std">y</span>

<span class="hl com"># calculate conditional expectation and marginal variances</span>
<span class="hl std">mu_x</span> <span class="hl kwb">=</span> <span class="hl kwd">drop</span><span class="hl std">(</span><span class="hl kwd">solve</span><span class="hl std">(Qx</span> <span class="hl opt">+</span> <span class="hl std">Qe, Qe</span> <span class="hl opt">%*%</span> <span class="hl std">y))</span>
<span class="hl std">sigma_x</span> <span class="hl kwb">=</span> <span class="hl kwd">sqrt</span><span class="hl std">(</span><span class="hl kwd">diag</span><span class="hl std">(</span><span class="hl kwd">solve</span><span class="hl std">(Qx</span> <span class="hl opt">+</span> <span class="hl std">Qe)))</span>

<span class="hl std">xy</span> <span class="hl kwb">=</span> <span class="hl std">xy</span> <span class="hl opt">%&gt;%</span> <span class="hl kwd">mutate</span><span class="hl std">(</span><span class="hl kwc">mu_x</span> <span class="hl std">= mu_x,</span> <span class="hl kwc">sigma_x</span> <span class="hl std">= sigma_x)</span>

<span class="hl kwd">ggplot</span><span class="hl std">(xy,</span> <span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">x</span><span class="hl std">=t))</span> <span class="hl opt">+</span>
  <span class="hl kwd">geom_ribbon</span><span class="hl std">(</span><span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">ymin</span><span class="hl std">=mu_x</span> <span class="hl opt">-</span> <span class="hl num">2</span> <span class="hl opt">*</span> <span class="hl std">sigma_x,</span>
                  <span class="hl kwc">ymax</span> <span class="hl std">= mu_x</span> <span class="hl opt">+</span> <span class="hl num">2</span> <span class="hl opt">*</span> <span class="hl std">sigma_x),</span>
        <span class="hl kwc">fill</span><span class="hl std">=</span><span class="hl str">'lightblue'</span><span class="hl std">)</span> <span class="hl opt">+</span>
  <span class="hl kwd">geom_line</span><span class="hl std">(</span><span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">y</span><span class="hl std">=x,</span> <span class="hl kwc">colour</span><span class="hl std">=</span><span class="hl str">'true latent x'</span><span class="hl std">))</span> <span class="hl opt">+</span>
  <span class="hl kwd">geom_line</span><span class="hl std">(</span><span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">y</span><span class="hl std">=y,</span> <span class="hl kwc">colour</span><span class="hl std">=</span><span class="hl str">'observed data y'</span><span class="hl std">))</span> <span class="hl opt">+</span>
  <span class="hl kwd">geom_line</span><span class="hl std">(</span><span class="hl kwd">aes</span><span class="hl std">(</span><span class="hl kwc">y</span><span class="hl std">=mu_x,</span> <span class="hl kwc">colour</span><span class="hl std">=</span><span class="hl str">'inferred latent x'</span><span class="hl std">))</span>
</pre></div>
</div><div class="rimage default"><img src="fig/rw2smooth.Rhtml-unnamed-chunk-5-1.png" title="plot of chunk unnamed-chunk-5" alt="plot of chunk unnamed-chunk-5" class="plot" /></div></div>


<h3>Inferring the hyperparameters</h3>

<p>
So far we have assumed that the hyperparameters \(\tau_x\) and \(\tau_e\) are known. 
In practice it's rare that we know the variances of the latent process or the measurement error, so we might want to estimate them.

<p>
The following approach calculates the posterior of the hyperparameters \(\theta = (\tau_x, \tau_e)\) given the observed data \(y\). 
It uses the two alternative factorisations of the joint density distribution 

\[ p(y,x,\theta) = p(y|x,\theta)p(x|\theta)p(\theta) = p(x|y,\theta)p(y|\theta)p(\theta)\]

and solves for \(p(\theta|y)\) treating all functions that don't depend on \(\theta\) as a multiplicative constant

\[\begin{aligned}
p(\theta | y) & = \frac{p(x,y,\theta)}{p(x|y,\theta)p(y)}\\
& = const \times \frac{p(y|x,\theta)p(x|\theta)p(\theta)}{p(x|y, \theta)}
\end{aligned}\]

<p>
One source of confusion is the occurrence of \(x\) on the rhs but not on the lhs, so we might ask, what configuration of \(x\) do we use on the rhs? The answer is, any configuration for which \(p(x|y,\theta)\) is non-zero. The conditional expectation \(x^* = E(x|y,\theta) = (Q_x + Q_e)^{-1}Q_e y\) is a natural choice.



